{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variaveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos de deep learning são representados por um grafo computacional, podemos ver um exemplo de grafo computacional na figura abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/compgraph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada círculo no grafo representa uma variável, e na entrada e saída de cada operação existe uma variável, no PyTorch, as variáveis são wrappers para os tensores, isso é, ela armazena os tensores e mais 3 atributos. O dado em si (data), o gradiente (grad) e um apontador (creator) para construir o grafo da backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/variable.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que faz o PyTorch ser um framework próprio para o uso em deep learning é sua capacidade de calcular o gradiente automaticamente de um grafo computacional definido, e isso no PyTorch é feito utilizando variáveis, ou seja, com as variáveis é possível criar expressões matemáticas e o PyTorch é capaz de calcular o gradiente dessa expressão.\n",
    "O gradiente é uma operação matemática que é calculada através de derivadas parciais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As variáveis são criadas a partir de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8807, 0.3669, 0.7776, 0.5670],\n",
       "        [0.9787, 0.2027, 0.1333, 0.1424]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Variable(torch.rand(2,4))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.4132, 0.7287, 0.0983, 4.2892],\n",
       "        [2.0199, 4.2254, 3.5996, 3.4344]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = 5*torch.rand(2,4)\n",
    "y = Variable(y_t)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**requires_grad** indica se o PyTorch deve ou não calcular o gradiente da variável, ou seja, se essa variável irá ser utilizada durante o treinamento no cálculo do gradiente. Por padrão *requires_grad* é falso quando a variável é criada, Se uma de suas entrada para um operação requer o cálculo do gradiente, sua saída e seus subgrafos também irão requerer o caclulo do gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24., 24.],\n",
       "        [24., 24.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "z = y * y * 2\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "\n",
    "# acessa o gradiente da variável x individualmente\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd é o pacote do PyTorch utilizado para calcular a derivada do grafo computacional, calculando o backpropagation começando pela variável que chamou a função *backward*. Nos modelos de deep learning, essa variável geralmente contém a função de perda.\n",
    "Para acessar o gradiente individual para cada variável é utilizado o atributo *grad*.\n",
    "\n",
    "O `backward()` destrói o grafo após sua execução. Isso é intrínseco ao PyTorch pelo fato dele trabalhar com grafos computacionais dinâmico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
